<!DOCTYPE 
HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en">   
<head>
  <title>ALTA Workshop</title>
  <meta http-equiv="content-type" content="text/html; charset=iso-8859-1">
  <link rel="stylesheet" href="alta_workshop.css" type="text/css">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54132983-1', 'auto');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>
</head>

<body>
<!-- This table contains the main structure of the page -->

<table width="100%" border="0" cellpadding="0" cellspacing="0" summary="This table contains the main structure">

  <tr valign="middle">
    <!-- Logo -->
    <td width="85" align="center"><a href="http://www.alta.asn.au"><img src="http://www.alta.asn.au/images/logo_sm.jpg" width="74" height="65" border="0" alt="ALTA logo"></a></td>
    <td></td>
    <!-- Title -->
    <td class="sitename" align="center">Australasian Language Technology Workshop 2014</td>
    <td></td>
    <td></td>
  </tr>

  <tr valign="top">


<td width="15%">

<!-- This table contains the Macquarie navigation links -->
<table width="100%" border="0" cellpadding="0" cellspacing="0" class="headerbar" summary="This table contains the top level navigation links">


  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="index.html">Home</a> 
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-importantdates.html">Important Dates</a>
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-cfp.html">Call for Papers</a> 
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-studenttravel.html">Student Travel Support</a> 
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-tutorials.html">Tutorials</a>
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-invitedtalks.html">Invited Talks</a>
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-committees.html">Committees</a>
    </td>
  </tr>

  <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-instructions.html">Instructions for Authors</a>
    </td>
 </tr>

   <tr>
    <td align="left" class = "headercell">
	
      <a class="headerlink" href="http://www.alta.asn.au/events/sharedtask2014/index.html">Shared Task</a>
    </td>
  </tr> 

 <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-acceptedpapers.html">Accepted Papers</a> 
    </td>
 </tr>

 <tr>
    <td align="left" class = "headercell">
	<a class="headerlink" href="alta-2014-reg.html">Registration</a> 
    </td>
 </tr>

 <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-accommodation.html">Accommodation</a> 
    </td>
 </tr>

 <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-programme.html">Programme</a> 
    </td>
 </tr>

 <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-proceedings.html">Proceedings</a>
    </td>
 </tr>

 <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-location.html">Location and travel</a>
    </td>
 </tr>

 <tr>
    <td align="left" class = "headercell">
      <a class="headerlink" href="alta-2014-pastalta.html">ALTA past workshops</a>
    </td>
 </tr>

<tr><td>&nbsp;</td></tr>
</table>
<!-- End Macquarie navigation table -->
</td>

<td width="2%">&nbsp;</td>


<td width="70%">

<!-- END OF HEADER - DO NOT EDIT ABOVE THIS LINE - Document Id: "altw"  -->


<center>
<h1>Tutorials at ALTA 2014</h1>
</center>

<h2>Tutorial 1</h2>
<p><b>Presenter:</b> Dr. Trevor Cohn and Daniel Beck</p>
<p><b>Title:</b> Gaussian Processes for NLP</p>
<p><b>Short Description:</b></p>
<p>This tutorial aims to cover the basic motivation, ideas and theory of Gaussian Processes and several applications to natural language processing tasks. Gaussian Processes (GPs) are a powerful modelling framework incorporating kernels and Bayesian inference, and are recognised as state-of-the-art for many machine learning tasks. This tutorial will focus primarily on regression and classification, both fundamental techniques of wide-spread use in the NLP community. We argue that the GP framework offers many benefits over commonly used machine learning frameworks, such as linear models (logistic regression, least squares regression) and support vector machines (SVMs). GPs have the advantage of being a fully Bayesian model, giving a posterior over the desired variables. Their probabilistic formulation allows for much wider applicability in larger graphical models, unlike SVMs. Moreover, several properties of Gaussian distributions means that GP (regression) supports analytic formulations for the posterior and predictive inference, avoiding the many approximation errors that plague approximate inference techniques in common use for Bayesian models (e.g. MCMCM, variational Bayes). GPs provide an elegant, flexible and simple means of probabilistic inference. GPs have been actively researched since the early 2000s, and are now reaching maturity: the fundamental theory and practice is well understood, and now research is focused into their applications, and improve inference algorithms, e.g. for scaling inference to large and high-dimensional datasets. Several open-source packages (e.g. GPy and GPML) have been developed which allow for GPs to be easily used for many applications. This tutorial aims to present the main ideas and theory behind GPs and recent applications to NLP, emphasising their potential for widespread application across many NLP tasks.</p>

<p>For more information, see <a href="http://people.eng.unimelb.edu.au/tcohn/tutorial.html">Trevor's page for the tutorial.</a>

<h2>Tutorial 2</h2>
<center><p><b>Unfortunately, due to unforeseen circumstances, this tutorial has been cancelled.</b></p></center>
<p><b>Presenter:</b> Dr. Gholamreza Haffari</p>
<p><b>Title:</b> Machine Learning Approaches for Dealing with Limited Bilingual Data in Statistical Machine Translation</p>
<p><b>Short Description:</b></p>
High quality translation output in Statistical machine translation (SMT) is dependent on the availability of massive amounts of parallel text in the source and target language. There are a large number of languages that are considered "low-density", either because the population speaking the language is not very large, or even if millions of people speak the language, insufficient online resources are available in that language. This tutorial covers machine learning approaches for dealing with such situations in SMT where the amount of available bilingual data is limited.
</p>
<!--
<p>We are pleased to announce that ALTA2014 will include pre-workshop tutorials on 4 December 2014. Schedules for these tutorials are available in the <a href="http://www.alta.asn.au/events/alta2014/alta-2014-programme.html">programme</a>.</p>

<h3>Working with the HCS vLab</h3>

<p><b>Dominique Estival (University of Western Sydney) and Steve Cassidy (Macquarie University)</b></p>

<p>
The Human Communication Science Virtual Laboratory (<a href="http://hcsvlab.org.au/">HCSvLab</a>) will run a half-day workshop. The HCSvLab provides an on-line infrastructure for accessing human communication corpora (speech, text, music, sounds, video, etc.) and for using specialised tools for searching, analysing and annotating that data.

The aims of the HCS vLab are to:
<ul>
<li> facilitate access of the Australian and international HCS communities to data and analysis tools;</li>
<li> afford new tool-corpus combinations and new emergent research;</li>
<li> allow analysis and annotation results to be stored and shared, thus promoting collaboration between institutions and disciplines;</li>
<li>improve scientific replicability by moving local and idiosyncratic desktop-based tools and data to an accessible, in-the-cloud, environment that standardises, defines, and captures procedures and data output so that research publications can be supported by re-runnable re-usable data and coded procedure.</li>
</ul>

The HCS vLab is designed to make use of national infrastructure - including data storage, discovery and research computing services. It incorporates existing eResearch tools, adapted to work on shared infrastructure, with a data-discovery interface to connect researchers with data sets, orchestrated by a workflow engine with both web and command line interfaces to allow use by technical and non-technical researchers, via a Web interface.
</p>

<h3>Applying Wikipedia as a machine-readable knowledge base</h3>
<p><b>David Milne (CSIRO)</b></p>

<p>
What if your search engine, recommender or clustering algorithm could
consult Wikipedia
as easily as we do, to understand more about the documents they encounter?

This is not a far-fetched idea. While clearly intended for human readers,
the raw structure
of the Wikipedia bears striking resemblance to traditional knowledge bases
and provides many
footholds for algorithms to extract machine-readable knowledge.
</p.
<p>
In this tutorial, we will work with Wikipedia to augment and enhance other
textual
information sources. This is broken down into three key problems:

<ul>
<li>  Extracting structured knowledge from Wikipedia;</li>
<li>  Connecting it to textual documents; and</li>
<li>  Allowing people to easily, effectively and intuitively tap into it while searching and browsing.</li>
</ul>
</p>
<p>
For each of the three problems described above we will provide live
demonstrations and hands-on
activities. For the extraction problem, we present an extremely large
thesaurus-like structure
that has been automatically generated from Wikipedia, and show how it can
be reasoned over
(in a rough fashion) by machines. For the connection task, we demonstrate
an algorithm that
can automatically detect and disambiguate Wikipedia topics when they are
mentioned in any
textual document, and intelligently predict those that are most likely of
interest to the reader.
For the final problem, we present several end-user applications that
combine the work described
above with slick visualisation techniques, to provide enhanced browsing
and searching experiences.
</p>
<p>
All of the presented systems are open source and publicly available on the
web.
</p>
-->

<tr>
<td colspan="6">

<!-- Logos -->

<!-- ----- FOOTER - DO NOT EDIT BEYOND THIS LINE ----- -->


</td>

<td width="2%">&nbsp;</td>

</tr>
</table>
<!-- End of main table -->

<!-- Start of footer -->
<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<!-- <td class="footertext"> -->
<td>
</td>
</tr>
</table>
<!-- End of footer -->
<hr>
&copy; ALTA 2014. <a href="mailto:workshop@alta.asn.au">Workshop Organisers</a>.
</body>
</html>
